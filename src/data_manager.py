#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Aè‚¡å…¨å¸‚åœºæ•°æ®ç®¡ç†æ¨¡å—
æ”¯æŒå…¨é‡è·å–ã€å¢é‡æ›´æ–°ã€æœ¬åœ°å­˜å‚¨
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import pickle
import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from data_client import StockDataClient

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StockDataManager:
    """Aè‚¡æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir="stock_data"):
        """
        åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.data_dir = data_dir
        self.client = StockDataClient()
        self.metadata_file = os.path.join(data_dir, "metadata.json")
        
        # åˆ›å»ºæ•°æ®ç›®å½•ç»“æ„
        self._create_directories()
        
        # åŠ è½½å…ƒæ•°æ®
        self.metadata = self._load_metadata()
        
        # çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤å…±äº«èµ„æº
        self._lock = threading.Lock()
        
        logger.info(f"æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®ç›®å½•: {self.data_dir}")
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            self.data_dir,
            os.path.join(self.data_dir, "daily"),      # æ—¥çº¿æ•°æ®
            os.path.join(self.data_dir, "stocks"),     # ä¸ªè‚¡æ•°æ®
            os.path.join(self.data_dir, "backup"),     # å¤‡ä»½æ•°æ®
            os.path.join(self.data_dir, "temp")        # ä¸´æ—¶æ–‡ä»¶
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def _load_metadata(self):
        """åŠ è½½å…ƒæ•°æ®"""
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                logger.info("å…ƒæ•°æ®åŠ è½½æˆåŠŸ")
                return metadata
            except Exception as e:
                logger.warning(f"å…ƒæ•°æ®åŠ è½½å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°çš„å…ƒæ•°æ®")
        
        # åˆ›å»ºé»˜è®¤å…ƒæ•°æ®
        return {
            "last_update": None,
            "stock_count": 0,
            "data_version": "1.0",
            "update_history": [],
            "failed_stocks": []
        }
    
    def _save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
            logger.debug("å…ƒæ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            logger.error(f"å…ƒæ•°æ®ä¿å­˜å¤±è´¥: {e}")
    
    def _get_today_str(self):
        """è·å–ä»Šæ—¥æ—¥æœŸå­—ç¬¦ä¸²"""
        return datetime.now().strftime("%Y%m%d")
    
    def _is_trading_day(self, date=None):
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸è€ƒè™‘å…·ä½“èŠ‚å‡æ—¥ï¼‰
        
        Args:
            date: æ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
        """
        if date is None:
            date = datetime.now()
        
        # å‘¨æœ«ä¸æ˜¯äº¤æ˜“æ—¥
        if date.weekday() >= 5:  # 5=å‘¨å…­, 6=å‘¨æ—¥
            return False
        
        # ç®€å•åˆ¤æ–­ï¼Œå®é™…åº”è¯¥ç»“åˆäº¤æ˜“æ‰€æ—¥å†
        return True
    
    def _should_update_data(self):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®"""
        if not self.metadata.get("last_update"):
            return True, "é¦–æ¬¡è·å–æ•°æ®"
        
        try:
            last_update = datetime.strptime(self.metadata["last_update"], "%Y%m%d")
            today = datetime.now()
            
            # å¦‚æœä¸æ˜¯äº¤æ˜“æ—¥ï¼Œæ£€æŸ¥æœ€è¿‘çš„äº¤æ˜“æ—¥
            if not self._is_trading_day(today):
                # æŸ¥æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥
                check_date = today
                for _ in range(7):  # æœ€å¤šå¾€å‰æ‰¾7å¤©
                    check_date -= timedelta(days=1)
                    if self._is_trading_day(check_date):
                        if last_update.date() >= check_date.date():
                            return False, "æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆéäº¤æ˜“æ—¥ï¼‰"
                        else:
                            return True, f"éœ€è¦æ›´æ–°åˆ°æœ€è¿‘äº¤æ˜“æ—¥: {check_date.strftime('%Y%m%d')}"
                return False, "æ— æ³•ç¡®å®šæœ€è¿‘äº¤æ˜“æ—¥"
            
            # å¦‚æœæ˜¯äº¤æ˜“æ—¥ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
            if last_update.date() < today.date():
                return True, f"æ•°æ®è¿‡æœŸï¼Œæœ€åæ›´æ–°: {self.metadata['last_update']}"
            
            return False, "æ•°æ®å·²æ˜¯æœ€æ–°"
            
        except Exception as e:
            logger.warning(f"åˆ¤æ–­æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")
            return True, "æ— æ³•åˆ¤æ–­æ›´æ–°çŠ¶æ€ï¼Œå¼ºåˆ¶æ›´æ–°"
    
    def get_all_stocks(self, force_update=False):
        """
        è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨
        
        Args:
            force_update: å¼ºåˆ¶æ›´æ–°
        """
        today_str = self._get_today_str()
        stock_list_file = os.path.join(self.data_dir, f"stock_list_{today_str}.pkl")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not force_update and os.path.exists(stock_list_file):
            try:
                logger.info(f"ğŸ“‚ åŠ è½½æœ¬åœ°è‚¡ç¥¨åˆ—è¡¨: {stock_list_file}")
                stocks = pd.read_pickle(stock_list_file)
                logger.info(f"âœ… æœ¬åœ°è‚¡ç¥¨åˆ—è¡¨åŠ è½½æˆåŠŸï¼Œå…± {len(stocks)} åªè‚¡ç¥¨")
                return stocks
            except Exception as e:
                logger.warning(f"æœ¬åœ°è‚¡ç¥¨åˆ—è¡¨åŠ è½½å¤±è´¥: {e}ï¼Œå°†é‡æ–°è·å–")
        
        logger.info("ğŸ”„ ä»æœåŠ¡å™¨è·å–æœ€æ–°è‚¡ç¥¨åˆ—è¡¨...")
        stocks = self.client.get_stock_list()
        
        if not stocks.empty:
            try:
                # ä¿å­˜åˆ°æœ¬åœ°
                stocks.to_pickle(stock_list_file)
                logger.info(f"ğŸ’¾ è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜: {stock_list_file}")
                logger.info(f"ğŸ“Š å…±è·å– {len(stocks)} åªè‚¡ç¥¨")
                
                # æ›´æ–°å…ƒæ•°æ®
                self.metadata["stock_count"] = len(stocks)
                self._save_metadata()
                
            except Exception as e:
                logger.error(f"ä¿å­˜è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
        else:
            logger.error("è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
        
        return stocks
    
    def download_stock_data(self, code, market, days=300, force_update=False):
        """
        ä¸‹è½½å•åªè‚¡ç¥¨æ•°æ®ï¼ˆæç®€ç‰ˆæœ¬ï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            market: å¸‚åœºä»£ç 
            days: è·å–å¤©æ•°
            force_update: å¼ºåˆ¶æ›´æ–°
        """
        # ç›´æ¥è·å–æ•°æ®ï¼Œä¸åšæœ¬åœ°ç¼“å­˜æ£€æŸ¥
        data = self.client.get_daily_data(code, market=market, count=days)
        
        if not data.empty:
            logger.debug(f"âœ… {code}: {len(data)} æ¡è®°å½•")
        else:
            logger.warning(f"âŒ {code}: æ— æ•°æ®")
        
        return data
    
    def download_all_market_data(self, force_update=False, max_stocks=None, batch_size=50, max_workers=10, end_date=None):
        """
        ä¸‹è½½å…¨å¸‚åœºæ•°æ®
        
        Args:
            force_update: å¼ºåˆ¶æ›´æ–°
            max_stocks: æœ€å¤§è‚¡ç¥¨æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            batch_size: æ‰¹å¤„ç†å¤§å°
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYYMMDD)
        """
        print("ğŸš€ å¼€å§‹è·å–Aè‚¡å…¨å¸‚åœºæ•°æ®")
        print("=" * 60)
        
        if end_date:
            end_date_str = pd.to_datetime(end_date, format='%Y%m%d').strftime('%Yå¹´%mæœˆ%dæ—¥')
            print(f"ğŸ“… æ•°æ®è·å–åˆ°: {end_date_str}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        need_update, reason = self._should_update_data()
        if not need_update and not force_update and not end_date:
            print(f"â„¹ï¸  {reason}")
            existing_data = self._load_existing_data()
            if existing_data:
                return existing_data
        
        print(f"ğŸ“… æ›´æ–°åŸå› : {reason}")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stocks = self.get_all_stocks(force_update)
        if stocks.empty:
            print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
            return {}
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if max_stocks and max_stocks > 0:
            stocks = stocks.head(max_stocks)
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: ä»…è·å–å‰ {max_stocks} åªè‚¡ç¥¨")
        
        print(f"ğŸ“Š å‡†å¤‡è·å– {len(stocks)} åªè‚¡ç¥¨çš„æ•°æ®")
        print(f"âš™ï¸  æ‰¹å¤„ç†å¤§å°: {batch_size}, çº¿ç¨‹æ•°: {max_workers}")
        
        # å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # åˆ†æ‰¹å¤„ç†
        all_data = {}
        failed_stocks = []
        successful_count = 0
        
        total_batches = (len(stocks) + batch_size - 1) // batch_size
        
        for i in range(0, len(stocks), batch_size):
            batch_stocks = stocks.iloc[i:i+batch_size]
            batch_num = i // batch_size + 1
            
            print(f"\nğŸ“¦ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹è‚¡ç¥¨ ({len(batch_stocks)} åª)")
            print("-" * 40)
            
            batch_start_time = time.time()
            batch_data, batch_failed = self._process_batch_threaded(batch_stocks, force_update, max_workers, end_date)
            batch_end_time = time.time()
            
            # åˆå¹¶ç»“æœ
            all_data.update(batch_data)
            failed_stocks.extend(batch_failed)
            successful_count += len(batch_data)
            
            # æ˜¾ç¤ºæ‰¹æ¬¡è¿›åº¦
            batch_success_rate = len(batch_data) / len(batch_stocks) * 100
            batch_time = batch_end_time - batch_start_time
            avg_time_per_stock = batch_time / len(batch_stocks)
            print(f"âœ… ç¬¬{batch_num}æ‰¹å®Œæˆ: {len(batch_data)}/{len(batch_stocks)} æˆåŠŸ "
                  f"({batch_success_rate:.1f}%), è€—æ—¶: {batch_time:.1f}ç§’ "
                  f"(å¹³å‡ {avg_time_per_stock:.2f}ç§’/åª)")
            
            # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
            total_progress = (i + len(batch_stocks)) / len(stocks) * 100
            elapsed_time = time.time() - start_time
            estimated_total_time = elapsed_time / total_progress * 100 if total_progress > 0 else 0
            remaining_time = estimated_total_time - elapsed_time
            
            print(f"ğŸ“ˆ æ€»è¿›åº¦: {successful_count}/{len(stocks)} "
                  f"({total_progress:.1f}%), å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’, "
                  f"é¢„è®¡å‰©ä½™: {remaining_time:.1f}ç§’")
            
            # åˆ é™¤æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œæœ€å¤§åŒ–ä¸‹è½½é€Ÿåº¦
        
        # ä¿å­˜å…¨é‡æ•°æ®
        if end_date:
            data_date = end_date
        else:
            data_date = self._get_today_str()
        
        all_data_file = os.path.join(self.data_dir, f"all_market_data_{data_date}.pkl")
        
        try:
            with open(all_data_file, 'wb') as f:
                pickle.dump(all_data, f)
            logger.info(f"å…¨å¸‚åœºæ•°æ®å·²ä¿å­˜: {all_data_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜å…¨å¸‚åœºæ•°æ®å¤±è´¥: {e}")
        
        # æ›´æ–°å…ƒæ•°æ®
        end_time = time.time()
        total_time = end_time - start_time
        
        update_date = data_date  # ä½¿ç”¨æ•°æ®æ—¥æœŸä½œä¸ºæ›´æ–°æ ‡è¯†
        
        self.metadata["last_update"] = update_date
        self.metadata["successful_stocks"] = successful_count
        self.metadata["failed_stocks"] = len(failed_stocks)
        self.metadata["update_history"].append({
            "date": update_date,
            "total_stocks": len(stocks),
            "successful": successful_count,
            "failed": len(failed_stocks),
            "duration": total_time,
            "is_historical": bool(end_date),  # æ ‡è®°æ˜¯å¦ä¸ºå†å²æ•°æ®
            "target_date": end_date if end_date else update_date
        })
        
        # ä¿ç•™æœ€è¿‘10æ¬¡æ›´æ–°è®°å½•
        if len(self.metadata["update_history"]) > 10:
            self.metadata["update_history"] = self.metadata["update_history"][-10:]
        
        self._save_metadata()
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        success_rate = successful_count / len(stocks) * 100
        print(f"\nğŸ‰ æ•°æ®è·å–å®Œæˆ!")
        print("=" * 60)
        print(f"âœ… æˆåŠŸè·å–: {successful_count} åªè‚¡ç¥¨ ({success_rate:.1f}%)")
        print(f"âŒ è·å–å¤±è´¥: {len(failed_stocks)} åªè‚¡ç¥¨")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {successful_count/total_time:.1f} åª/ç§’")
        print(f"ğŸ’¾ æ•°æ®æ–‡ä»¶: {all_data_file}")
        
        if failed_stocks:
            print(f"\nâš ï¸  å¤±è´¥è‚¡ç¥¨åˆ—è¡¨ (å‰10ä¸ª): {failed_stocks[:10]}")
            if len(failed_stocks) > 10:
                print(f"    ... è¿˜æœ‰ {len(failed_stocks)-10} åªè‚¡ç¥¨è·å–å¤±è´¥")
        
        return all_data
    
    def _process_batch(self, batch_stocks, force_update):
        """å¤„ç†å•æ‰¹è‚¡ç¥¨"""
        batch_data = {}
        failed_stocks = []
        
        for idx, row in batch_stocks.iterrows():
            code = row['code']
            name = row.get('name', code)
            market = 1 if str(code).startswith('6') else 0
            
            try:
                data = self.download_stock_data(code, market, days=300, force_update=force_update)
                if not data.empty:
                    batch_data[code] = {
                        'data': data,
                        'info': {
                            'code': code,
                            'name': name,
                            'market': market,
                            'update_time': datetime.now().isoformat(),
                            'data_count': len(data)
                        }
                    }
                    print(f"âœ… {code} ({name}) - {len(data)} æ¡è®°å½•")
                else:
                    failed_stocks.append(code)
                    print(f"âŒ {code} ({name}) - æ•°æ®ä¸ºç©º")
                
            except Exception as e:
                failed_stocks.append(code)
                print(f"âŒ {code} ({name}) - è·å–å¤±è´¥: {e}")
                continue
            
            # åˆ é™¤è¯·æ±‚é—´éš”ï¼Œæœ€å¤§åŒ–ä¸‹è½½é€Ÿåº¦
        
        return batch_data, failed_stocks
    
    def _process_batch_threaded(self, batch_stocks, force_update, max_workers=10, end_date=None):
        """
        ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†å•æ‰¹è‚¡ç¥¨
        
        Args:
            batch_stocks: è‚¡ç¥¨æ‰¹æ¬¡æ•°æ®
            force_update: å¼ºåˆ¶æ›´æ–°
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYYMMDD)
        """
        batch_data = {}
        failed_stocks = []
        
        # åˆ›å»ºçº¿ç¨‹æ± 
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_stock = {}
            for idx, row in batch_stocks.iterrows():
                code = row['code']
                name = row.get('name', code)
                market = 1 if str(code).startswith('6') else 0
                
                future = executor.submit(self._download_single_stock_safe, code, name, market, force_update, end_date)
                future_to_stock[future] = (code, name)
            
            # æ”¶é›†ç»“æœ
            completed_count = 0
            for future in as_completed(future_to_stock):
                code, name = future_to_stock[future]
                completed_count += 1
                
                try:
                    result = future.result()
                    if result['success']:
                        with self._lock:  # ä¿æŠ¤å…±äº«æ•°æ®
                            batch_data[code] = result['data']
                        print(f"âœ… {code} ({name}) - {result['data']['info']['data_count']} æ¡è®°å½• ({completed_count}/{len(batch_stocks)})")
                    else:
                        with self._lock:
                            failed_stocks.append(code)
                        print(f"âŒ {code} ({name}) - {result['error']} ({completed_count}/{len(batch_stocks)})")
                        
                except Exception as e:
                    with self._lock:
                        failed_stocks.append(code)
                    print(f"âŒ {code} ({name}) - çº¿ç¨‹å¼‚å¸¸: {e} ({completed_count}/{len(batch_stocks)})")
        
        return batch_data, failed_stocks
    
    def _download_single_stock_safe(self, code, name, market, force_update, end_date=None):
        """
        å®‰å…¨åœ°ä¸‹è½½å•åªè‚¡ç¥¨æ•°æ®ï¼ˆæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹å®¢æˆ·ç«¯ï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            name: è‚¡ç¥¨åç§°  
            market: å¸‚åœºæ ‡è¯†
            force_update: å¼ºåˆ¶æ›´æ–°
            end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: YYYYMMDD)
            
        Returns:
            dict: åŒ…å«æˆåŠŸæ ‡å¿—å’Œæ•°æ®çš„å­—å…¸
        """
        try:
            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„å®¢æˆ·ç«¯å®ä¾‹ï¼Œé¿å…å…±äº«å†²çª
            from data_client import StockDataClient
            thread_client = StockDataClient()
            
            # åªé‡è¯•2æ¬¡ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
            for attempt in range(2):
                try:
                    # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´å‚æ•°
                    if attempt == 0:
                        # ç¬¬ä¸€æ¬¡ï¼šæ ‡å‡†å‚æ•°
                        data = thread_client.get_daily_data(code, market=market, count=300)
                    else:
                        # ç¬¬äºŒæ¬¡ï¼šå‡å°‘æ•°æ®é‡ï¼Œæ›´å¿«è·å–
                        data = thread_client.get_daily_data(code, market=market, count=100)
                    
                    # å¦‚æœæŒ‡å®šäº†ç»“æŸæ—¥æœŸï¼Œéœ€è¦ç­›é€‰æ•°æ®
                    if not data.empty and end_date:
                        try:
                            # ç¡®ä¿æ—¥æœŸåˆ—å­˜åœ¨å¹¶è½¬æ¢æ ¼å¼
                            if 'æ—¥æœŸ' in data.columns:
                                data['æ—¥æœŸ'] = pd.to_datetime(data['æ—¥æœŸ'], format='%Y%m%d', errors='coerce')
                                target_date = pd.to_datetime(end_date, format='%Y%m%d')
                                data = data[data['æ—¥æœŸ'] <= target_date]
                        except Exception as e:
                            print(f"âš ï¸  {code} æ—¥æœŸç­›é€‰å‡ºé”™: {e}")
                    
                    if not data.empty:
                        # å¿«é€ŸéªŒè¯æ•°æ®è´¨é‡
                        if 'æ”¶ç›˜' in data.columns and len(data) > 0:
                            latest_close = data['æ”¶ç›˜'].iloc[-1]
                            if pd.notna(latest_close) and latest_close > 0:
                                stock_info = {
                                    'data': data,
                                    'info': {
                                        'code': code,
                                        'name': name,
                                        'market': market,
                                        'update_time': datetime.now().isoformat(),
                                        'data_count': len(data),
                                        'end_date': end_date if end_date else datetime.now().strftime('%Y%m%d')
                                    }
                                }
                                
                                return {
                                    'success': True,
                                    'data': stock_info
                                }
                
                    # é‡è¯•å‰æçŸ­ç­‰å¾…ï¼Œå¤šçº¿ç¨‹ä¸‹å‡å°‘å†²çª
                    if attempt == 0:
                        time.sleep(0.01)  # åªç­‰å¾…10æ¯«ç§’
                        
                except Exception as e:
                    # åªåœ¨ç¬¬ä¸€æ¬¡å¤±è´¥æ—¶çŸ­æš‚ç­‰å¾…
                    if attempt == 0:
                        time.sleep(0.01)
                    continue
            
            return {
                'success': False,
                'error': 'æ— æ•°æ®'
            }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def _load_existing_data(self):
        """åŠ è½½ç°æœ‰æ•°æ®"""
        today_str = self._get_today_str()
        all_data_file = os.path.join(self.data_dir, f"all_market_data_{today_str}.pkl")
        
        if os.path.exists(all_data_file):
            try:
                logger.info(f"ğŸ“‚ åŠ è½½ç°æœ‰æ•°æ®: {all_data_file}")
                with open(all_data_file, 'rb') as f:
                    data = pickle.load(f)
                logger.info(f"âœ… ç°æœ‰æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(data)} åªè‚¡ç¥¨")
                return data
            except Exception as e:
                logger.warning(f"ç°æœ‰æ•°æ®åŠ è½½å¤±è´¥: {e}")
        
        return {}
    
    def get_market_data(self, date=None):
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„å¸‚åœºæ•°æ®
        
        Args:
            date: æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤ä»Šå¤©
        """
        if date is None:
            date = self._get_today_str()
        
        data_file = os.path.join(self.data_dir, f"all_market_data_{date}.pkl")
        
        if os.path.exists(data_file):
            try:
                with open(data_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.error(f"åŠ è½½æ•°æ®å¤±è´¥ {date}: {e}")
        
        return {}
    
    def list_available_data(self):
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®æ–‡ä»¶"""
        print("ğŸ“‹ å¯ç”¨æ•°æ®æ–‡ä»¶:")
        print("-" * 40)
        
        data_files = []
        for filename in os.listdir(self.data_dir):
            if filename.startswith("all_market_data_") and filename.endswith(".pkl"):
                date_str = filename.replace("all_market_data_", "").replace(".pkl", "")
                file_path = os.path.join(self.data_dir, filename)
                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                
                data_files.append((date_str, filename, file_size))
        
        # æŒ‰æ—¥æœŸæ’åº
        data_files.sort(key=lambda x: x[0], reverse=True)
        
        if data_files:
            for date_str, filename, file_size in data_files:
                formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                print(f"ğŸ“… {formatted_date}: {filename} ({file_size:.1f} MB)")
        else:
            print("æš‚æ— å¯ç”¨æ•°æ®æ–‡ä»¶")
    
    def clean_old_data(self, keep_days=7):
        """
        æ¸…ç†æ—§æ•°æ®
        
        Args:
            keep_days: ä¿ç•™å¤©æ•°
        """
        cutoff_date = datetime.now() - timedelta(days=keep_days)
        cutoff_str = cutoff_date.strftime("%Y%m%d")
        
        print(f"ğŸ§¹ æ¸…ç† {cutoff_str} ä¹‹å‰çš„æ•°æ®...")
        
        removed_count = 0
        total_size = 0
        
        # æ¸…ç†ä¸»æ•°æ®æ–‡ä»¶
        for filename in os.listdir(self.data_dir):
            if ("all_market_data_" in filename or "stock_list_" in filename) and filename.endswith(".pkl"):
                date_str = filename.split("_")[-1].replace(".pkl", "")
                if len(date_str) == 8 and date_str < cutoff_str:
                    file_path = os.path.join(self.data_dir, filename)
                    file_size = os.path.getsize(file_path) / (1024 * 1024)
                    total_size += file_size
                    os.remove(file_path)
                    print(f"ğŸ—‘ï¸  åˆ é™¤: {filename} ({file_size:.1f} MB)")
                    removed_count += 1
        
        # æ¸…ç†ä¸ªè‚¡æ•°æ®
        stocks_dir = os.path.join(self.data_dir, "stocks")
        if os.path.exists(stocks_dir):
            for filename in os.listdir(stocks_dir):
                if filename.endswith(".pkl"):
                    parts = filename.split("_")
                    if len(parts) >= 2:
                        date_str = parts[-1].replace(".pkl", "")
                        if len(date_str) == 8 and date_str < cutoff_str:
                            file_path = os.path.join(stocks_dir, filename)
                            try:
                                file_size = os.path.getsize(file_path) / (1024 * 1024)
                                total_size += file_size
                                os.remove(file_path)
                                removed_count += 1
                            except:
                                pass
        
        print(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤äº† {removed_count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {total_size:.1f} MB ç©ºé—´")
    
    def get_data_statistics(self):
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š æ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
        print("-" * 40)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        print(f"ğŸ“… æœ€åæ›´æ–°: {self.metadata.get('last_update', 'æœªæ›´æ–°')}")
        print(f"ğŸ“ˆ è‚¡ç¥¨æ•°é‡: {self.metadata.get('stock_count', 0)}")
        
        # æ•°æ®æ–‡ä»¶ç»Ÿè®¡
        total_files = 0
        total_size = 0
        
        for root, dirs, files in os.walk(self.data_dir):
            for file in files:
                if file.endswith('.pkl'):
                    file_path = os.path.join(root, file)
                    try:
                        size = os.path.getsize(file_path) / (1024 * 1024)
                        total_size += size
                        total_files += 1
                    except:
                        pass
        
        print(f"ğŸ“¦ æ•°æ®æ–‡ä»¶: {total_files} ä¸ª")
        print(f"ğŸ’¾ æ€»å¤§å°: {total_size:.1f} MB")
        
        # æ›´æ–°å†å²
        update_history = self.metadata.get('update_history', [])
        if update_history:
            print(f"\nğŸ“‹ æœ€è¿‘æ›´æ–°è®°å½•:")
            for record in update_history[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5æ¬¡
                date = record['date']
                formatted_date = f"{date[:4]}-{date[4:6]}-{date[6:8]}"
                success_rate = record['successful'] / record['total_stocks'] * 100
                print(f"   {formatted_date}: {record['successful']}/{record['total_stocks']} "
                      f"({success_rate:.1f}%), è€—æ—¶ {record['duration']:.1f}ç§’")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    print("ğŸ¯ Aè‚¡å…¨å¸‚åœºæ•°æ®ç®¡ç†ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    manager = StockDataManager()
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    manager.get_data_statistics()
    
    # åˆ—å‡ºå¯ç”¨æ•°æ®
    print("\n")
    manager.list_available_data()
    
    # è·å–å…¨å¸‚åœºæ•°æ®ï¼ˆæµ‹è¯•æ¨¡å¼ï¼Œåªè·å–å‰20åªè‚¡ç¥¨ï¼‰
    print(f"\nğŸš€ å¼€å§‹è·å–æ•°æ®...")
    all_data = manager.download_all_market_data(max_stocks=20)
    
    print(f"\nğŸ“ˆ æ•°æ®è·å–ç»“æœ:")
    for i, (code, stock_info) in enumerate(list(all_data.items())[:5]):
        data = stock_info['data']
        info = stock_info['info']
        print(f"   {code} ({info['name']}): {len(data)} æ¡è®°å½•")

if __name__ == "__main__":
    main()
